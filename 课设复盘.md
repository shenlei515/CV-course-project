## 过程

5.25-5.27 看论文，想idea（看论文时间可以加快点）

5.28-5.30 看源码（real-ESRGAN的源码还是比较困难的）

5.31-6.1 改代码(这样一想改代码的时间确实不多，其实大块完整时间就前天晚上九点半后加昨天一天)

7.4-7.6



中间没有做课设的时间：

5.25下午 吃饭，休息

5.26早上下午晚上 睡觉，DDPG调试

5.28晚上 去唱歌了

5.30下午和晚上九点半前 信息论实验



## 做的好的地方

idea想的比较快，并且找的两篇论文都有源码



## 暴露的问题

1. 效率太低(体现在看源码的速度和写代码的速度)
   1. 原因：分心，代码能力不足，没有条理的看源码，没有早运行源码来验证想法
2. 工作时间过短
   1. 作息不规律导致早上时间被浪费
3. 代码基础弱
   1. 卷积层参数和过程不熟悉
   2. 对于注册器不熟悉
   3. 大型代码debug能力不足
   4. 对于训练过程用到的pytorch类型不了解(dataloader, DataParallel)
   5. 对于并行训练不了解
4. 总想着偷懒
   1. Real-ESRGAN看起来就很难修改，但为了少写代码(想避开模型搭建和权重加载过程，以为会节约时间)，但是Real-ESRGAN这种较为出名的模型为了通用性和性能做了很多封装，实际上我们需要的功能可能只是其中一种，要在这种代码上进行修改实在太过困难(当然读懂并修改这种代码能收获许多)
   2. 懒得重写训练过程于是直接用了原来的trainloader，但是发现它根本不能满足我们对于数据集的要求

## debug日志



显存不够，将batch_size从None(全部输进去)变成2(一次每张图片输入两张)

batch要从第二维分，比较复杂

并行训练导致输入被分成了三份，而ConditionNet的特征没有被分开(第一维度是零)



Condition_loss为nan,先看看是contrastive loss和sr_support_loss哪部分loss为nan

loss情况

contrastive_loss:::::::::: nan
sr_support_loss:::::::::: inf
condition_loss:::::::::: nan

第二轮就出现这种情况，第一轮还是正常的

再看了一些和loss相关的变量的情况

half_condition_feature1：全是nan

half_condition_feature2：也全是nan

sr_support_x1：全是inf

也就是说与网络相关的全部变成了nan

Condition权重全是nan

去掉.half()后，不会有nan的情况(应该是.half()忽略了一些较小的数，导致数值溢出等问题)

